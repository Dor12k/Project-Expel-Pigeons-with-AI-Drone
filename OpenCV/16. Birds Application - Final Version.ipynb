{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac3cd88",
   "metadata": {},
   "source": [
    "## **CNN in Real Time using Transfer Learning with ResNet50 on CIFAR10 Dataset** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3036a62",
   "metadata": {},
   "source": [
    "This app recognize movments, **Detect Object** and **Tracking** after it in **Real Time Camera.**                                  \n",
    "We use **Resnet50** model with **Transfer Learning** on **CIFAR10** Dataset that we trained and perfoms **95% accuracy.**       \n",
    "After we recognize movment we detect the object by using our model and then we start tracking after it.                         \n",
    "We store all the detections events in format of date and time and and show it on the screen.                                     \n",
    "The application using **TensorFlow** with **Keras** by **Python**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e9bd3",
   "metadata": {},
   "source": [
    "### Import Libraries, Loading Model and Set Camera  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efa13bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               25690368  \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,348,490\n",
      "Trainable params: 49,294,346\n",
      "Non-trainable params: 54,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import h5py\n",
    "import winsound\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import strftime\n",
    "from collections import deque\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the labels of CIFAR-10 datasest\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Load our model that trained by 25 epochs on CIFAR dataset\n",
    "model = load_model(r'C:\\file location..')\n",
    "\n",
    "# Print trained model summery\n",
    "model.summary()\n",
    "\n",
    "# Catch frame from webcam\n",
    "camera = cv2.VideoCapture(r'C:\\file location..')\n",
    "\n",
    "# This path is location for the sound file\n",
    "soundPath = (r'C:\\file location..') \n",
    "\n",
    "# This path is loaction for the saved images \n",
    "outPutPath = (r'C:\\file location..') \n",
    "\n",
    "# This path is location for the saved detections events\n",
    "txtPath = (r'C:\\file location..')\n",
    "\n",
    "# Writing The Headline of the text file\n",
    "with open(txtPath, 'a') as f:\n",
    "    f.write(\"###################### Detection Events #########################\")\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a82b31",
   "metadata": {},
   "source": [
    "### Define The Application Constans Variabels ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93077492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to decrease the frame size\n",
    "SCALE = 1\n",
    "\n",
    "# Define variables for hight and width shape of the frames\n",
    "HEIGH, WIDTH = 350, 500   \n",
    "\n",
    "# Define variables for hight and width shape to prediction model input\n",
    "hight, width = 224, 224  \n",
    "\n",
    "# Define objects boundaries size\n",
    "MIN_OBJECT_AREA = 1000\n",
    "MAX_OBJECT_AREA = 10000\n",
    "\n",
    "# Define the tresh hold of the masks\n",
    "DIFF_TRESH_HOLD = 20  # Should be low\n",
    "MASK_TRESH_HOLD = 100 # Should be high\n",
    "\n",
    "# Variables for start rows and cols to put text\n",
    "FIRST_ROW = int(HEIGH/10)\n",
    "FIRST_COL = int(WIDTH/10)\n",
    "ROWS_SPACE = int(HEIGH/10) \n",
    "\n",
    "# Define the font size as precent from the screen size\n",
    "FONT_SIZE = ((HEIGH/1500))\n",
    "\n",
    "# Create backgroung of the main frame\n",
    "foregroundModel = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Define the background\n",
    "last_frame = np.zeros((int(HEIGH/SCALE), int(WIDTH/SCALE),3) , np.uint8)\n",
    "scores_frame = np.zeros((int(HEIGH/SCALE), int(WIDTH/SCALE),3) , np.uint8)\n",
    "\n",
    "# Initializing deque object for center points of the detected object\n",
    "points = deque(maxlen=50)\n",
    "\n",
    "# Define timer to check the tracking\n",
    "tracking_check = 10\n",
    "\n",
    "# Define tracker dictionary\n",
    "tracker_dict = { 'csrt': cv2.TrackerCSRT_create,\n",
    "                 'kcf' : cv2.TrackerKCF_create,\n",
    "                 'boosting' : cv2.legacy.TrackerMOSSE_create(),\n",
    "                 'mil': cv2.TrackerMIL_create,\n",
    "                 'tld': cv2.legacy.TrackerTLD_create(),\n",
    "                 'medianflow': cv2.legacy.TrackerMedianFlow_create(),\n",
    "                 'mosse':cv2.legacy.TrackerMOSSE_create()}\n",
    "\n",
    "# Initialize our tracker after the object\n",
    "tracker = tracker_dict['csrt']()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774eceff",
   "metadata": {},
   "source": [
    "### Define Application Variables ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05d9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store first frame from reading camera\n",
    "ret, frm = camera.read()\n",
    "\n",
    "# Inititialize first frames\n",
    "cut_fragment_rgb = np.zeros((HEIGH, WIDTH, 3), np.uint8)\n",
    "tracking_frame = np.zeros((HEIGH, WIDTH, 3), np.uint8)\n",
    "inf_frame = np.zeros((HEIGH, WIDTH, 3), np.uint8)\n",
    "\n",
    "# Define counters variables\n",
    "counter_images_processing = 0\n",
    "counter_frames_processing = 0\n",
    "counter_frames_prediction = 0\n",
    "counter_fail_predictions = 0\n",
    "counter_birds_prediction = 0 \n",
    "counter_frames_tracking = 0\n",
    "counter_frames_reading = 0    \n",
    "\n",
    "# Variable store the system status of tracking or not tracking\n",
    "tracking_on = False\n",
    "\n",
    "# Restart timer for FPS\n",
    "fps_start = timer()  \n",
    "\n",
    "# Inintialize variables\n",
    "detection_time = 0\n",
    "counter_fps = 0\n",
    "FPS = 0\n",
    "\n",
    "# Variable hold all the detections events\n",
    "log = []\n",
    "label = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137f169",
   "metadata": {},
   "source": [
    "### Define The General Application Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51160e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function return 3-Dimension frame\n",
    "def expands_dimensions(frame):\n",
    "    \n",
    "    new_image = np.zeros((frame.shape[0], frame.shape[1], 3), np.uint8)\n",
    "    new_image[:, :, 0] = frame\n",
    "    new_image[:, :, 1] = frame\n",
    "    new_image[:, :, 2] = frame\n",
    "    \n",
    "    return new_image\n",
    "\n",
    "# Convert frame from rgb to gray\n",
    "def gray_frame(frame_rgb):\n",
    "    \n",
    "    # Converting captured frame to GRAY by OpenCV function    \n",
    "    gray_frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    frame_gray = np.zeros(frame_rgb.shape, np.uint8)\n",
    "    frame_gray[:,:,0] = gray_frame\n",
    "    frame_gray[:,:,1] = gray_frame\n",
    "    frame_gray[:,:,2] = gray_frame\n",
    "    \n",
    "    return frame_gray\n",
    "\n",
    "# Function return empty frame for initlize the main window\n",
    "def empty_frame(frame_bgr):\n",
    "    \n",
    "    empy_frame = np.zeros(frame_bgr.shape, np.uint8)\n",
    "    empy_frame[:, :, 0] = 255\n",
    "    empy_frame[:, :, 1] = 255\n",
    "    empy_frame[:, :, 2] = 255\n",
    "    \n",
    "    return empy_frame\n",
    "\n",
    "# Function return how many mitnutes passed between two detections\n",
    "def detection_timer(detection_time, last_time):\n",
    "    \n",
    "    if 0 < len(last_time):    \n",
    "        element = last_time\n",
    "        minute = element[len(element) -2]\n",
    "        minute = int(minute) * 10\n",
    "        second = element[len(element) -1]\n",
    "        second = int(second)\n",
    "        minutes = minute + second\n",
    "        last_time_minutes = minutes\n",
    "\n",
    "        element = detection_time\n",
    "        minute = element[len(element) -2]\n",
    "        minute = int(minute) * 10\n",
    "        second = element[len(element) -1]\n",
    "        second = int(second)\n",
    "        minutes = minute + second\n",
    "        detection_time_minutes = minutes\n",
    "\n",
    "        time_past = abs(detection_time_minutes - last_time_minutes)\n",
    "    \n",
    "        return time_past\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function create 3 frames from the frame we read\n",
    "def preproccess_frames(frame, last_frame):\n",
    "    \n",
    "    \n",
    "    # Resize the main frame to (WIDTH, HEIGH) shape\n",
    "    frame = cv2.resize(frame, (WIDTH, HEIGH))\n",
    "        \n",
    "    # Copy frame to work with deffrent variable\n",
    "    frame_rgb = cv2.resize(frame, (int(WIDTH/SCALE), int(HEIGH/SCALE)))\n",
    "    \n",
    "    # Return mask frame \n",
    "    frame_mask = cv_mask(frame_rgb, last_frame)\n",
    "    \n",
    "    # Return mask for tracking\n",
    "    tracking_mask = mask_tracking(frame_rgb, last_frame)\n",
    "    \n",
    "    # Define last frame\n",
    "    last_frame = frame_rgb.copy()\n",
    "    \n",
    "    return frame, frame_rgb, tracking_mask, frame_mask, last_frame\n",
    "\n",
    "# Function define the mask\n",
    "def mask(frame_rgb, background):\n",
    "    \n",
    "    # Converting captured frame to GRAY by OpenCV function    \n",
    "    frame_gray = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Create one more frame with Gaussian blur\n",
    "    frame_gray = cv2.GaussianBlur(frame_gray, (25, 25), 0)  \n",
    "\n",
    "    # Converting captured frame to GRAY by OpenCV function        \n",
    "    background = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create one more frame with Gaussian blur\n",
    "    background = cv2.GaussianBlur(background, (25, 25), 0)    \n",
    "    \n",
    "    # Return mask to detect change between two frames   \n",
    "    abs_diff = cv2.absdiff(frame_gray, background)\n",
    "    \n",
    "    # Function exclude values that ara more than treshhold = 15 0 and more than 255\n",
    "    _, mask = cv2.threshold(abs_diff, 20, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Dilates the object in the frame\n",
    "    dilated_mask = cv2.dilate(mask, None, iterations = 5) \n",
    "    \n",
    "    return dilated_mask\n",
    "\n",
    "# Function create a mask to track the object\n",
    "def mask_tracking(frame_RGB, last_frame):\n",
    "    \n",
    "    # Converting captured frame to GRAY by OpenCV function    \n",
    "    frame_gray = cv2.cvtColor(frame_RGB, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Converting captured frame to GRAY by OpenCV function        \n",
    "    last_frame = cv2.cvtColor(last_frame, cv2.COLOR_BGR2GRAY)\n",
    "     \n",
    "    # Return mask to detect change between two frames   \n",
    "    abs_diff = cv2.absdiff(frame_gray, last_frame)\n",
    "    \n",
    "    # Function exclude values that ara more than treshhold = 15 0 and more than 255\n",
    "    _, abs_diff_mask = cv2.threshold(abs_diff, DIFF_TRESH_HOLD, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Expend mask dimension to 3 dimension\n",
    "    mask_frame = expands_dimensions(abs_diff_mask)        \n",
    "    \n",
    "    return mask_frame\n",
    "\n",
    "# Function create a mask with connectedComponents\n",
    "def cv_mask(frame_rgb, last_frame):\n",
    "       \n",
    "    # Apply the frame to forground model\n",
    "    foreground_mask = foregroundModel.apply(frame_rgb)\n",
    "    \n",
    "    # Reduce noises\n",
    "    structuring_element = cv2.getStructuringElement(cv2.MORPH_RECT,(3,3))\n",
    "    foreground_mask = cv2.morphologyEx(np.float32(foreground_mask), cv2.MORPH_OPEN, structuring_element)\n",
    "    \n",
    "    # Find out connected components and keep only the large components\n",
    "    num_labels, image_labels = cv2.connectedComponents(np.array(0<foreground_mask, np.uint8))\n",
    "    \n",
    "    # Return components larger than threshold\n",
    "    foreground_mask = keepLargeComponents(image_labels, treshold=0) \n",
    "    \n",
    "    # Using 'clip' function to exclude values that are less than 0 and more than 255\n",
    "    foreground_mask = np.clip(foreground_mask, 0, 255).astype(np.uint8) \n",
    "    \n",
    "    # Function exclude values that ara more than treshhold = 15 0 and more than 255\n",
    "    _, foreground_mask = cv2.threshold(foreground_mask, 0, 255, cv2.THRESH_BINARY)   \n",
    "    \n",
    "    # Converting output feature map from Tensor to Numpy array\n",
    "    foreground_mask = foreground_mask[:, :, np.newaxis]  \n",
    "        \n",
    "    return foreground_mask\n",
    "\n",
    "# This function remove the components that are smaller than praticular threshold\n",
    "def keepLargeComponents(image, treshold):\n",
    "    \n",
    "    frame = np.zeros(image.shape) < 0 # boolean array\n",
    "    unique_labels = np.unique(image.flatten()) # find out every unique value that is actually a label \n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label == 0: # background\n",
    "            pass\n",
    "        else:\n",
    "            img = (image == label) # save the component\n",
    "            if treshold < np.sum(img):\n",
    "                frame = frame | img # save all the components\n",
    "                \n",
    "    return np.float32(255*frame)\n",
    "\n",
    "# Function plot bar chart with scores values\n",
    "def bar_chart(obtained_scores, classes_names):\n",
    "    \n",
    "    # Arranging X axis\n",
    "    x_positions = np.arange(obtained_scores.size)\n",
    "\n",
    "    # Creating bar chart\n",
    "    bars = plt.bar(x_positions, obtained_scores, align='center', alpha=0.6)\n",
    "\n",
    "    # Highlighting the highest bar\n",
    "    bars[np.argmax(obtained_scores)].set_color('red')\n",
    "\n",
    "    # Giving labels to bars along X axis\n",
    "    plt.xticks(x_positions, classes_names, rotation=25, fontsize=10)\n",
    "\n",
    "    # Giving names to axes\n",
    "    plt.xlabel('Class', fontsize=20)\n",
    "    plt.ylabel('Value', fontsize=20)\n",
    "\n",
    "    # Giving name to bar chart\n",
    "    plt.title('Obtained Scores', fontsize=20)\n",
    "\n",
    "    # Adjusting borders of the plot\n",
    "    plt.tight_layout(pad=2.5)\n",
    "\n",
    "    # Initializing object of the buffer\n",
    "    b = io.BytesIO()\n",
    "\n",
    "    # Saving bar chart into the buffer\n",
    "    plt.savefig(b, format='png', dpi=200)\n",
    "\n",
    "    # Closing plot with bar chart\n",
    "    plt.close()\n",
    "\n",
    "    # Moving pointer to the beginning of the buffer\n",
    "    b.seek(0)\n",
    "\n",
    "    # Reading bar chart from the buffer\n",
    "    bar_image = np.frombuffer(b.getvalue(), dtype=np.uint8)\n",
    "\n",
    "    # Closing buffer\n",
    "    b.close()\n",
    "\n",
    "    # Decoding buffer\n",
    "    bar_image = cv2.imdecode(bar_image, 1)\n",
    "\n",
    "    # Resize frame to HEIGH X WIDTH\n",
    "    bar_image = cv2.resize(bar_image, (WIDTH, HEIGH))\n",
    "\n",
    "    # Returning Numpy array with bar chart\n",
    "    return bar_image\n",
    "\n",
    "# Function get 3 frames and collect them to 1 frame \n",
    "def collaction_frames(left_frame, mid_frame, right_frame):\n",
    "\n",
    "    # Insert all frames to array for scan it\n",
    "    frames = [left_frame, mid_frame, right_frame]\n",
    "\n",
    "    # Change all frames to 3 chanels    \n",
    "    for i in range(len(frames)):\n",
    "        \n",
    "        # Check if frames[i][3] is exis\n",
    "        if len(frames[i].shape) < 3:\n",
    "            \n",
    "            # Adding 3-Dimension to the image\n",
    "            frames[i] = frames[i][:,:,np.newaxis]\n",
    "        \n",
    "        # Find frames that not 3 channels\n",
    "        if frames[i].shape[2] != 3:\n",
    "             \n",
    "            # Function expand mask's dimension from 1 to 3 odimensions    \n",
    "            frames[i] = np.repeat(frames[i], 3, axis=2)\n",
    "                \n",
    "    # Define frames in the right order\n",
    "    left_frame = frames[0]\n",
    "    mid_frame = frames[1]\n",
    "    right_frame = frames[2]\n",
    "    \n",
    "    # Create one window that contain: frame, tracking, mask\n",
    "    collaction_frame = np.hstack((left_frame, mid_frame, right_frame))\n",
    "    \n",
    "    return collaction_frame\n",
    "\n",
    "# Function takes 6 windows and collect them to one main window\n",
    "def build_main_window(frame, inf_frame, tracking_frame, cut_fragment_rgb, scores_frame, frame_mask):\n",
    "    \n",
    "    # Function return one window that contain (frame_bgr, track_frame, mask_frame)\n",
    "    upper_window = collaction_frames(frame, inf_frame, tracking_frame)\n",
    "\n",
    "    # Function return one window that contain (cut_fragment_bgr_frame, scores_frame, mask_frame)\n",
    "    lower_window = collaction_frames(cut_fragment_rgb, scores_frame, frame_mask)\n",
    "\n",
    "    # Create one window that contain: upper_window and lower_window\n",
    "    main_window = np.vstack((upper_window, lower_window))    \n",
    "    \n",
    "    return main_window\n",
    "\n",
    "# Function display the frames on the screen in one window\n",
    "def display_windows(frame, tracking_mask, frame_mask):  \n",
    "        \n",
    "    # Copy frame to work with deffrent variable\n",
    "    tracking_mask = cv2.resize(tracking_mask, (WIDTH, HEIGH))\n",
    "        \n",
    "    # Copy frame to work with deffrent variable\n",
    "    frame_mask = cv2.resize(frame_mask, (WIDTH, HEIGH))\n",
    "        \n",
    "    # Expend mask dimension to 3 dimension\n",
    "    frame_mask = expands_dimensions(frame_mask)       \n",
    "    \n",
    "    # Create left window    \n",
    "    main_window = np.hstack((frame, tracking_mask, frame_mask))\n",
    "\n",
    "    # Plotting all the frames in one window\n",
    "    cv2.imshow(\"Main_Window\", main_window)     \n",
    "    \n",
    "# Function manage the frames reader variables like fps etc'\n",
    "def reader_manger(FPS, fps_start, counter_fps, tracking_on):\n",
    "       \n",
    "    # Variable says if keep reading frame or quit\n",
    "    quit = False\n",
    "\n",
    "    # Stopping the timer for FPS\n",
    "    fps_stop = timer()\n",
    "\n",
    "    # Print FPS every 1 second\n",
    "    if 1.0 <= fps_stop - fps_start:\n",
    "\n",
    "        # Define FPS\n",
    "        FPS = counter_fps\n",
    "\n",
    "        # Reset FPS counter\n",
    "        counter_fps = 0\n",
    "\n",
    "        # Restart timer for FPS\n",
    "        fps_start = timer()       \n",
    "\n",
    "    # Function waits for key to be pressed    \n",
    "    key = cv2.waitKey(10) % 256\n",
    "\n",
    "    # If 'n' is pressed, we catchs the frame and define it as the background\n",
    "    if key == ord('n'):\n",
    "        tracking_on = False\n",
    "\n",
    "    # If 'q' key is pressed then quit from app\n",
    "    if key == ord('q'):\n",
    "        quit = True   \n",
    "\n",
    "    return FPS, fps_start, counter_fps, tracking_on, quit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476516bd",
   "metadata": {},
   "source": [
    "### Define the Tracking and Detection Objects functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc92e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function manage the detection and return status and coordinates\n",
    "def detection_manager(trackers, counters, frames):\n",
    "    \n",
    "    # Initialize label variables\n",
    "    label = \"\"\n",
    "    scores = []\n",
    "    detection_time = 0   \n",
    "    prediction_timer = 0 \n",
    "    tracking_on = False  \n",
    "    \n",
    "    # Extract function variables   \n",
    "    tracker = trackers[0]\n",
    "    frame, frame_mask, cut_fragment_rgb, scores_frame = [f for f in frames]\n",
    "    counter_frames_prediction, counter_birds_prediction = [c for c in counters]\n",
    "\n",
    "    # Function return array of all contours we found\n",
    "    contours, _ = cv2.findContours(frame_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sorted the contours and define the larger first\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    # Scan the contours list\n",
    "    for contour in contours:\n",
    "\n",
    "        # Return square area of the given contour\n",
    "        contour_area = cv2.contourArea(contour)\n",
    "\n",
    "        # Find contours between MIN_OBJECT_AREA to MAX_OBJECT_AREA\n",
    "        if contour_area < MAX_OBJECT_AREA:\n",
    "            if MIN_OBJECT_AREA < contour_area:\n",
    "\n",
    "                # Increase prediction counter\n",
    "                counter_frames_prediction += 1                \n",
    "                \n",
    "                # Function return the cutted rgb fragment in model input_shape\n",
    "                cut_fragment_rgb, rectangle = cut_fragment(frame, contour)\n",
    "                \n",
    "                # Function predict the cutted fragment and return the result\n",
    "                scores, label, prediction_timer, detection_time = prediction_manager(cut_fragment_rgb)    \n",
    "                \n",
    "                # Start traking after the detected bird\n",
    "                if label == \"bird\": \n",
    "                    \n",
    "                    # Increasing Birds prediction counter\n",
    "                    counter_birds_prediction += 1\n",
    "\n",
    "                    # Function add object to the tracker, save detection and drawing rectangle\n",
    "                    scores_frame, tracker = detected_bird(frame, tracker, rectangle, detection_time, label, scores)\n",
    "                                                    \n",
    "                    # Update tracking status\n",
    "                    tracking_on = True\n",
    "                    break\n",
    "\n",
    "                # End of if not detected birds        \n",
    "                else: \n",
    "                    # Increasing fail predictions counter\n",
    "                    counter_fail_predictions = counter_frames_prediction - counter_birds_prediction              \n",
    "            else:\n",
    "                # Contour is a sorted list so all the rest items irrelevant\n",
    "                break\n",
    "    \n",
    "    # Compress the function variable into an arrays\n",
    "    frames = [frame, frame_mask, cut_fragment_rgb, scores_frame]\n",
    "    detected = [label, scores, tracker, tracking_on, prediction_timer, detection_time]\n",
    "    counters = [counter_frames_prediction, counter_birds_prediction]\n",
    "    \n",
    "    return frames, detected, counters\n",
    "\n",
    "# Function cut the detected fragment and return it\n",
    "def cut_fragment(frame, contour):\n",
    "                \n",
    "    # Get an approximate rectangle coordinates\n",
    "    (x_min, y_min, box_width, box_height) = cv2.boundingRect(contour)\n",
    "\n",
    "    # bounding_boxes contain x1, y1, x2, y2, coordinates and not width and heigh\n",
    "    (x_min, y_min, box_width, box_height) = x_min*SCALE, y_min*SCALE, box_width*SCALE, box_height*SCALE\n",
    "    \n",
    "    # Cutting detected fragment from BGR frame\n",
    "    cut_fragment_rgb_frame = frame[y_min: y_min + box_height, x_min: x_min + box_width]\n",
    "    \n",
    "    # Resize the fragment to the right frame shape in the main window\n",
    "    cut_fragment_rgb_frame = cv2.resize(cut_fragment_rgb_frame, (WIDTH, HEIGH))\n",
    "    \n",
    "    # Create rectangle object from the boundaries coordinates\n",
    "    rectangle = np.array([x_min, y_min, box_width, box_height])\n",
    "\n",
    "    return cut_fragment_rgb_frame, rectangle\n",
    "\n",
    "# Function manage the prediction part and return prediction results\n",
    "def prediction_manager(cut_fragment_rgb):\n",
    "\n",
    "    # Measuring classification time\n",
    "    start = timer()\n",
    "\n",
    "    # Function return all scores of model predictions\n",
    "    scores = prediction_model(cut_fragment_rgb)       \n",
    "\n",
    "    # End of Measuring classification time\n",
    "    end = timer()\n",
    "                             \n",
    "    # Current time of detection object\n",
    "    detection_time = strftime(\"%d/%m/%Y %H:%M:%S\")  \n",
    "\n",
    "    # Calculate the time that needed to predict the fragment\n",
    "    prediction_timer = end - start\n",
    "    \n",
    "    # Finds the lables array index by the max score index of model prediction\n",
    "    index = np.argmax(scores)\n",
    "\n",
    "    # Define the label for the cut_fragment from labels array\n",
    "    label = labels[index]    \n",
    "    \n",
    "    return scores, label, prediction_timer, detection_time\n",
    "\n",
    "# Function predict model's output from the cutted fragment    \n",
    "def prediction_model(cut_fragment_rgb):\n",
    "        \n",
    "    # Create a copy of the cut_fragment_bgr frame  \n",
    "    fragment = cut_fragment_rgb.copy()\n",
    "    \n",
    "    # Resizing frame to the right shape of the model's input\n",
    "    fragment = cv2.resize(fragment, (width, hight), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Extending dimension from (height, width, channels) to (1, heigh, width, channels)\n",
    "    fragment = fragment[np.newaxis, :, :, :]\n",
    "\n",
    "    # Predict score from model\n",
    "    scores = model.predict(fragment)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Function drawing rectangle around the predicted object\n",
    "def drawing_rectangle(frame, rectangle, label):\n",
    "\n",
    "    # Get an approximate rectangle coordinates\n",
    "    (x_min, y_min, box_width, box_height) = [int(a) for a in rectangle]\n",
    "\n",
    "    # Drawing bounding box on the current BGR frame\n",
    "    cv2.rectangle(frame, (x_min, y_min), (x_min + box_width, y_min + box_height), (0, 255, 0), 3)\n",
    "\n",
    "    # Putting text with label on the current BGR frame\n",
    "    cv2.putText(frame, label, (x_min - 5, y_min - 25), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "    return frame \n",
    "\n",
    "# Function initialize variables of bird detected\n",
    "def detected_bird(frame, tracker, rectangle, detection_time, label, scores):\n",
    "    \n",
    "    # Initialize Deque points\n",
    "    points.clear()\n",
    "\n",
    "    # Makes a sound to alert about bird\n",
    "    #winsound.PlaySound(soundPath, winsound.SND_FILENAME)\n",
    "                                \n",
    "    # Function return a bgr frame with rectangle around the cut fragment\n",
    "    frame = drawing_rectangle(frame, rectangle, label)  \n",
    "    \n",
    "    # Save detection event in date-time format\n",
    "    save_detection_event(frame, detection_time, label)\n",
    "\n",
    "    # Function return a frame with all the labels scores     \n",
    "    scores_frame = bar_chart(scores[0], labels)  \n",
    "\n",
    "    # Add the detected object to the tracker \n",
    "    tracker.init(frame, rectangle)   \n",
    "    \n",
    "    return scores_frame, tracker\n",
    "    \n",
    "# Function manage the tracking and return the status\n",
    "def tracking_manager(tracking_function_vraiables):     \n",
    "\n",
    "    # Set the default status\n",
    "    tracking_on = False    \n",
    "    \n",
    "    # Extract the function arrays\n",
    "    tracking_frames = tracking_function_vraiables[1]   \n",
    "    tracking_counter = tracking_function_vraiables[2]\n",
    "    tracking_variables = tracking_function_vraiables[0]\n",
    "    \n",
    "    # Extract the function variable\n",
    "    tracker, scores, labels = [t for t in tracking_variables]\n",
    "    frame, frame_mask, tracking_mask = [f for f in tracking_frames]\n",
    "    prediction_timer, counter_frames_reading, FPS = [c for c in tracking_counter]\n",
    "\n",
    "    # Get the bounding box from the frame\n",
    "    (success, contour_box) = tracker.update(frame)\n",
    "\n",
    "    # Keep tracking after the object\n",
    "    if success:\n",
    "\n",
    "        # Get the coordinates of the rectangle around the object\n",
    "        (x, y, w, h) = [int(a) for a in contour_box]\n",
    "\n",
    "        # Check if coordinates is in the frame boundaries\n",
    "        if 0 <= x and x+w <= WIDTH and 0 <= y and y+h <= HEIGH: \n",
    "\n",
    "            # Set tracking status ON\n",
    "            tracking_on = True                 \n",
    "\n",
    "            # Cut the fragment from the mask frame\n",
    "            cut_fragment_track = tracking_mask[int(y/SCALE):int((y+h)/SCALE), int(x/SCALE):int((x+w)/SCALE)] \n",
    "\n",
    "            # Cut the fragment from the mask frame\n",
    "            cut_fragment_mask = frame_mask[int(y/SCALE):int((y+h)/SCALE), int(x/SCALE):int((x+w)/SCALE)] \n",
    "            \n",
    "            # Checking if tracking is still running after the object or not\n",
    "            if counter_frames_reading%tracking_check == 0: \n",
    "\n",
    "                # Calculate the pixels values sum, zero means background\n",
    "                if np.sum(cut_fragment_track) == 0:\n",
    "\n",
    "                    # Set tracking status OFF\n",
    "                    tracking_on = False      \n",
    "            \n",
    "            # Function return a frame with the tracking of the cut fragment\n",
    "            tracking_frame = drawing_tracking(frame, contour_box, scores, tracking_on, prediction_timer, FPS +1)      \n",
    "\n",
    "            # Drawing bounding box on the current BGR frame        \n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (100,255,0), 2)\n",
    "\n",
    "            # Putting text with label on the current BGR frame\n",
    "            cv2.putText(frame, label, (x - 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)                          \n",
    "            \n",
    "    # Function return a frame with the tracking of the cut fragment\n",
    "    tracking_frame = drawing_tracking(frame, contour_box, scores, tracking_on, prediction_timer, FPS +1) \n",
    "    \n",
    "    return frame, tracking_frame, tracker, tracking_on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3d880",
   "metadata": {},
   "source": [
    "### Define the Windows and Information Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b410f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function save the frame of the detected event\n",
    "def save_detection_event(frame, detection_time, label):\n",
    "\n",
    "    # Create a copy of time object\n",
    "    timer = detection_time\n",
    "    \n",
    "    # Save detection_time in format that fit to files\n",
    "    timer = timer[:2] + \"-\" + timer[3:5] + \"-\" + timer[6:13] + \"-\" + timer[14:16] + \"-\" + timer[17:]\n",
    "\n",
    "    # Define the name of the image\n",
    "    image_name = label + ' ' + str(timer)+'.jpg'\n",
    "    \n",
    "    # Define the file adress\n",
    "    finalPath = os.path.join(outPutPath, image_name)\n",
    "    \n",
    "    # Save the frame of detection event\n",
    "    cv2.imwrite(finalPath, frame)\n",
    "\n",
    "    # Checking if there is detection\n",
    "    if 0 < len(label): \n",
    "        \n",
    "        # Order all line to the same length\n",
    "        if len(label) < len(\"airplane\"):\n",
    "            label = str(label) + (len(\"airplane\") - len(label))*\" \"\n",
    "            \n",
    "        # Create string of detection event in date-time format\n",
    "        detection = \"Detected \" + str(label) + \" at: \" + detection_time\n",
    "        \n",
    "    # Writing the detection event into the file in txtPath location\n",
    "    with open(txtPath, 'a') as f:\n",
    "        f.write(detection)\n",
    "        f.write('\\n')\n",
    "        \n",
    "# Function create frame that follow the object movement\n",
    "def drawing_tracking(frame, contour_box, scores, tracking_on, prediction_timer, FPS):\n",
    "           \n",
    "    # Variable for ain boundries\n",
    "    shift_left = int(WIDTH/5) +10\n",
    "    shift_down = int(HEIGH -FIRST_ROW)\n",
    "    \n",
    "    # Define the prediction time for fragment\n",
    "    model_prediction_time = prediction_timer\n",
    "    \n",
    "    # Get the coordinates of the rectangle around the object\n",
    "    (x_min, y_min, box_width, box_height) = [int(a) for a in contour_box]\n",
    "    \n",
    "    # Getting current center coordinates of the bounding box\n",
    "    center = (int(x_min + box_width / 2), int(y_min + box_height / 2))\n",
    "\n",
    "    # Adding current point to the queue\n",
    "    points.appendleft(center)                        \n",
    "        \n",
    "    # Creating image with black background\n",
    "    track_frame = np.zeros(frame.shape, np.uint8)\n",
    "\n",
    "    # Changing background to Black color\n",
    "    track_frame[:, :, 0] = 0\n",
    "    track_frame[:, :, 1] = 0\n",
    "    track_frame[:, :, 2] = 0\n",
    "\n",
    "    # Visualizing tracker line\n",
    "    for i in range(1, len(points)):\n",
    "        \n",
    "        # If no points collected yet\n",
    "        if points[i - 1] is None or points[i] is None:\n",
    "            continue\n",
    "\n",
    "        # Draw the line between points\n",
    "        cv2.line(track_frame, points[i - 1], points[i], (50, 200, 50), 2)\n",
    "\n",
    "    # Adding text with center coordinates of the bounding box\n",
    "    cv2.putText(track_frame, 'X: {0}'.format(center[0]), (FIRST_COL -25, FIRST_ROW +10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*4, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(track_frame, 'Y: {0}'.format(center[1]), (FIRST_COL -25, FIRST_ROW + 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*4, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Adding text with time spent for 2D convolution for current frame\n",
    "    cv2.putText(track_frame, 'Time : ' + '{0:.3f}'.format(model_prediction_time), (FIRST_COL -25, HEIGH -40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*3, (255, 255, 255), 2, cv2.LINE_AA)  \n",
    "\n",
    "    # Adding text with score of convolution for current frame\n",
    "    cv2.putText(track_frame, 'Score : ' + '{0:.3f}'.format(scores[0][np.argmax(scores)]), (FIRST_COL -25, HEIGH -10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*3, (255, 255, 255), 2, cv2.LINE_AA)  \n",
    "\n",
    "    # Adding text with current label on the frame\n",
    "    cv2.putText(track_frame, \"FPS: \" + str(FPS), (WIDTH -FIRST_COL*3, HEIGH -FIRST_ROW +10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*4, (255, 255, 255), 3, cv2.LINE_AA)\n",
    "       \n",
    "    # If Tracking is on - put text on frame\n",
    "    if tracking_on:\n",
    "        \n",
    "        # Adding text with tracking status on the frame\n",
    "        cv2.putText(track_frame, 'Tracking ', (WIDTH -FIRST_COL*3, FIRST_ROW +20), \n",
    "                    cv2.FONT_HERSHEY_TRIPLEX, FONT_SIZE*3, (50, 200, 50), 1, cv2.LINE_AA)\n",
    "        \n",
    "    # Delete the \"Tracking\" alert from the screen    \n",
    "    if not tracking_on:\n",
    "\n",
    "        track_frame[WIDTH -135:, 0:FIRST_ROW + 15, 0] = 0\n",
    "        track_frame[WIDTH -135:, 0:FIRST_ROW + 15, 1] = 0\n",
    "        track_frame[WIDTH -135:, 0:FIRST_ROW + 15, 2] = 0\n",
    "        \n",
    "        track_frame[center[1], center[0], 0] = 0\n",
    "        track_frame[center[1], center[0], 1] = 0\n",
    "        track_frame[center[1], center[0], 2] = 0\n",
    "        \n",
    "        points.clear()\n",
    "\n",
    "    return track_frame\n",
    " \n",
    "# Function create the information frame in the main window\n",
    "def info_frame(inf_frame, fps, counter_images_reading, counter_images_processing, counter_images_tracking, time, label):\n",
    "    \n",
    "    left_boundary = int(FIRST_COL -40)\n",
    "    row_space = int(ROWS_SPACE/2) +5\n",
    "    \n",
    "    # Variable for ain boundries\n",
    "    shift_left = int(WIDTH/5) +10\n",
    "    shift_down = int(HEIGH -FIRST_ROW)\n",
    "        \n",
    "    # Define the font size as 2*FONT_SIZE=(HEIGHT/1000)\n",
    "    font_size = (FONT_SIZE*3)\n",
    "    \n",
    "    # Initilize frame with white background\n",
    "    inf_frame = empty_frame(inf_frame)\n",
    "    \n",
    "    # Variable hold amount images processing in percentage\n",
    "    processing_percentage = str(int((counter_images_processing * 100) / counter_images_reading))+\"%\"\n",
    "    tracking_percentage = str(int((counter_images_tracking * 100) / counter_images_reading))+\"%\"\n",
    "    \n",
    "    # Adding text with right time for current frame\n",
    "    cv2.putText(inf_frame, strftime(\"%H:%M:%S\"), (WIDTH -shift_left, shift_down), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Adding text with right date for current frame\n",
    "    cv2.putText(inf_frame, strftime(\"%d/%m/%Y\"), (3*shift_left+20, shift_down +30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)  \n",
    "    \n",
    "    # Adding text with Model name of the app\n",
    "    cv2.putText(inf_frame, \"Model: ResNet50\", (left_boundary, FIRST_ROW), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)  \n",
    "    \n",
    "    # Adding text with the training datset of the app\n",
    "    cv2.putText(inf_frame, \"Dataset: Cifar10\", (left_boundary, FIRST_ROW +row_space), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)  \n",
    "    \n",
    "    # Adding text with Camera FPS for current frame\n",
    "    cv2.putText(inf_frame, 'Camera FPS: ' + '{0:.0f}'.format(fps), (left_boundary, FIRST_ROW + (row_space*2)), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)  \n",
    "    \n",
    "    # Adding text with reading frames counter for current frame\n",
    "    cv2.putText(inf_frame, 'Reading Frames: ' + '{0:.0f}'.format(counter_images_reading),(left_boundary, FIRST_ROW +(row_space*3)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA) \n",
    "    \n",
    "    # Adding text with time spent for 2D convolution for current frame\n",
    "    cv2.putText(inf_frame, 'Tracking Images: ' + tracking_percentage, (left_boundary, FIRST_ROW +(row_space*4)), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA) \n",
    "    \n",
    "    # Adding text with processing images counter for current frame\n",
    "    cv2.putText(inf_frame, 'Processing Images: ' + processing_percentage, (left_boundary, FIRST_ROW +(row_space*5)), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA) \n",
    "        \n",
    "    # Define the next line where the detections text will start\n",
    "    end_text_line = (FIRST_ROW +(row_space*6)) +15\n",
    "    \n",
    "    # Function put text of all detections events on the frame\n",
    "    plotting_detections(inf_frame, label, time, end_text_line)\n",
    "    \n",
    "    return inf_frame\n",
    "\n",
    "# Function coninue info_frame Function and put text of detection in info frame\n",
    "def plotting_detections(inf_frame, label, time, start_text_line):\n",
    "\n",
    "    # Define the font size as 2*FONT_SIZE=(HEIGHT/1000)\n",
    "    font_size = (FONT_SIZE*3) -0.1\n",
    "    \n",
    "    # Find max label string length\n",
    "    max_len = len('airplane')\n",
    "                     \n",
    "    # Checking if there is detection\n",
    "    if 0 < len(label): \n",
    "\n",
    "        # Order all line to the same length\n",
    "        if len(label) < max_len:\n",
    "            label = str(label) + (max_len - len(label))*\" \"\n",
    "            \n",
    "        detection = \"Detected \" + str(label) + \" at: \" + str(time)\n",
    "\n",
    "        # Insert first element to log array\n",
    "        if len(log) == 0:\n",
    "            log.append(detection)\n",
    "        else:\n",
    "            # Check there ara different time of detection event\n",
    "            if detection != log[-1]:\n",
    "                  \n",
    "                # Calculate the how many minutes passed from the last detection    \n",
    "                time_past_in_minutes = detection_timer(time, log[-1])\n",
    "                log.append(detection)\n",
    "    \n",
    "    # Line number we start write objects we detected\n",
    "    line = start_text_line\n",
    "    \n",
    "    # Define the end line bounderies\n",
    "    end_line = (HEIGH - FIRST_ROW -20)\n",
    "    \n",
    "    # Scan all the detections object to plot them on the frame\n",
    "    for i in range(len(log)):\n",
    "        \n",
    "        # Variable represent the detection event as a string\n",
    "        event = log[i]\n",
    "        \n",
    "        # Check frames boundaries including text\n",
    "        if end_line <= line:\n",
    "            \n",
    "            # Delete old detection by set pixels to while and initilize line to start\n",
    "            inf_frame[start_text_line -20:end_line,:] = 255\n",
    "            \n",
    "            # Back to line 130\n",
    "            line = start_text_line\n",
    "        \n",
    "        # Adding text with DETECTION EVENT for current frame\n",
    "        cv2.putText(inf_frame, event, (10, line), cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 255), 1, cv2.LINE_AA) \n",
    "        \n",
    "        # skip to the next line\n",
    "        line += int(HEIGH/10) - 5  \n",
    "        \n",
    "# Function writes all the app's information into a text file\n",
    "def write_info_txt(counter_frames_reading, counter_frames_tracking, counter_frames_processing, counter_images_processing, \n",
    "                   counter_frames_predictions, counter_birds_predictions):\n",
    "    \n",
    "    counter_fail_predictions = counter_frames_predictions -counter_birds_predictions\n",
    "    counter_frames_not_processing = counter_frames_processing -counter_images_processing\n",
    "    \n",
    "    # Stores all the application's information in array and then writes them into a file\n",
    "    app_info = []\n",
    "    app_info.append(str(\"\\n###################### Application Information ##################\\n\"))\n",
    "    app_info.append(str(\"counter frames reading: \" + str(counter_frames_reading)))\n",
    "    app_info.append(str(\"counter frames tracking: \" + str(counter_frames_tracking)))\n",
    "    app_info.append(str(\"counter frames processing: \" + str(counter_frames_processing)))\n",
    "    app_info.append(str(\"counter images processing: \" + str(counter_images_processing)))\n",
    "    app_info.append(str(\"counter frames predictons: \" + str(counter_frames_predictions)))\n",
    "    app_info.append(str(\"counter birds predictions: \" + str(counter_birds_predictions)))\n",
    "    app_info.append(str(\"counter fail predictions: \" + str(counter_fail_predictions)))\n",
    "    app_info.append(str(\"counter frames not processing: \" + str(counter_frames_not_processing)))\n",
    "    app_info.append(str(\" \"))\n",
    "    app_info.append(str(\"Tracking Frames: \"+ str(int((counter_frames_tracking * 100) /counter_frames_reading))+\"%\"))\n",
    "    app_info.append(str(\"Processing Frames: \"+str(int((counter_frames_processing * 100) /counter_frames_reading))+\"%\"))\n",
    "    app_info.append(str(\"Right Predictions: \"+'{0:.0f}'.format((counter_birds_predictions*100)/counter_frames_predictions)+\"%\"))\n",
    "    app_info.append(str(\" \"))\n",
    "    app_info.append(str(\"############################## END ################################\\n\\n\"))\n",
    "\n",
    "    # Writing all application's information into a text file\n",
    "    with open(txtPath, 'a') as f:\n",
    "        f.write('\\n'.join(app_info))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873f627",
   "metadata": {},
   "source": [
    "### Images Processing and Model Predictions ###\n",
    "\n",
    "Reading frames from camera frame by frame and recognize movments to detect object in the frame. The detected object is cutted from the original frame and resize to the model input shape. The cutted object sent to model and gives us the label prediction with the score. After we detected the object we drawing green rectangle around it and start to tracking after the object. We display to the user one window that divide to six small windows:\n",
    "\n",
    "1. The frame that we read from camera with the rectangle around the object and the label above it.\n",
    "2. Window that contain information of the image processing like model's name, detection event, time, etc'.\n",
    "3. Window that draw a line that present the tracking of the detected object.\n",
    "4. Frame of the object we cutted form the original frame and sent to the model.\n",
    "5. Window that show us graph with all the labels and their score from predicted object.\n",
    "6. Window that display us the mask we use to detect object and movements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a6b785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 738ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n"
     ]
    }
   ],
   "source": [
    "#### Loop reading frame by frame and processing them\n",
    "while True:\n",
    "    \n",
    "    # Capturing frames one-by-one from camera\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    # If the frame was not retrieved then we break the loop\n",
    "    if not ret or frame is None:\n",
    "        break\n",
    "    \n",
    "    # Increasing frames counter\n",
    "    counter_frames_reading += 1\n",
    "    \n",
    "    # Define the fps of the loop using cv2 function\n",
    "    fps = int(camera.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Function return 3 diffrent kind of frames\n",
    "    frame, frame_rgb, tracking_mask, frame_mask, last_frame = preproccess_frames(frame, last_frame)\n",
    "    \n",
    "    # Start Proccessing state only if is not quite frames\n",
    "    if 0 < np.sum(tracking_mask):\n",
    "    \n",
    "        # Increasing FPS counter\n",
    "        counter_fps += 1\n",
    "        \n",
    "        # Increasing images counter\n",
    "        counter_images_processing +=1\n",
    "        \n",
    "        # Start Tracking state\n",
    "        if tracking_on == True:\n",
    "            \n",
    "            if counter_frames_reading%1 == 0:\n",
    "\n",
    "                # Increase tracking counter\n",
    "                counter_frames_tracking += 1\n",
    "\n",
    "                # Compress the fucntion variables into an array\n",
    "                tracking_variables = [tracker, scores, labels]\n",
    "                tracking_frames    = [frame, frame_mask, tracking_mask]\n",
    "                tracking_counter   = [prediction_timer, counter_frames_reading, FPS]\n",
    "                tracking_function_vraiables = [tracking_variables, tracking_frames, tracking_counter]\n",
    "\n",
    "                # Function manage the the tracking part  \n",
    "                frame, tracking_frame, tracker, tracking_on = tracking_manager(tracking_function_vraiables)\n",
    "\n",
    "        # Start Detection state\n",
    "        else:\n",
    "            # Increase tracking counter\n",
    "            counter_frames_processing += 1         \n",
    "            \n",
    "            # Compres function's variables into an array\n",
    "            trackers = [tracker]\n",
    "            counters = [counter_frames_prediction, counter_birds_prediction]\n",
    "            frames   = [frame, frame_mask, cut_fragment_rgb, scores_frame]\n",
    "                        \n",
    "            # Function find countors around the objects and return drawn frame\n",
    "            frames, detected, counters = detection_manager(trackers, counters, frames)\n",
    "             \n",
    "            # Extract the variables that returned from detection_manager function\n",
    "            frame, frame_mask, cut_fragment_rgb, scores_frame = [frame for frame in frames]\n",
    "            label, scores, tracker, tracking_on, prediction_timer, detection_time = [var for var in detected]\n",
    "            counter_frames_prediction, counter_birds_prediction = [counter for counter in counters]\n",
    "\n",
    "            \n",
    "        # Function return frame that contain the app stats and informations\n",
    "        inf_frame = info_frame(inf_frame, fps, counter_frames_reading, counter_frames_processing, counter_frames_tracking, \n",
    "                               detection_time, label)\n",
    "\n",
    "        # Create one window that contain: upper_window and lower_window\n",
    "        main_window = build_main_window(frame, inf_frame, tracking_frame, cut_fragment_rgb, scores_frame, frame_mask) \n",
    "\n",
    "        # Function manage the frames reader variables\n",
    "        FPS, fps_start, counter_fps, tracking_on, quit = reader_manger(FPS, fps_start, counter_fps, tracking_on)   \n",
    "\n",
    "        # Plotting all the frames in one window\n",
    "        cv2.imshow(\"Main_Window\", main_window)     \n",
    "\n",
    "        # If quit is true so we stop read frames\n",
    "        if quit == True:\n",
    "            break\n",
    "    \n",
    "# Releasing camera\n",
    "camera.release()\n",
    "\n",
    "# Destroying all opened OpenCV windows\n",
    "cv2.destroyAllWindows()      \n",
    "\n",
    "# Function write all the app's information into a txt file\n",
    "write_info_txt(counter_frames_reading, counter_frames_tracking, counter_frames_processing, \n",
    "               counter_images_processing, counter_frames_prediction, counter_birds_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
